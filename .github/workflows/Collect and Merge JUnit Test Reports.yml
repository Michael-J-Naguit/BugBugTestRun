name: Collect and Merge JUnit Test Reports

on:
  workflow_dispatch:

permissions:
  actions: read  # Needed to list and download artifacts
  contents: read  # Required to read repo content

jobs:
  collect-artifacts:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install jq and coreutils
        run: sudo apt-get install -y jq coreutils

      - name: List and Download Artifacts from a Specific Workflow (Last 30 Days)
        run: |
          mkdir -p all-test-results
          echo "Fetching artifacts from workflow: My-Specific-Workflow (last 30 days)"

          # Get the current date and calculate the cutoff date (30 days ago)
          CURRENT_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          CUTOFF_DATE=$(date -u -d "30 days ago" +"%Y-%m-%dT%H:%M:%SZ")
          echo "Current Date: $CURRENT_DATE"
          echo "Cutoff Date: $CUTOFF_DATE"

          # Fetch workflow runs for the specific workflow name
          WORKFLOW_ID=$(gh workflow list --json id,name --jq '.[] | select(.name=="Run BugBug - QRA Stage Suite") | .id' --repo ${{ github.repository }})

          if [ -z "$WORKFLOW_ID" ]; then
            echo "No workflow found with the name 'Run BugBug - QRA Stage Suite'"
            exit 1
          fi

          # Get all completed runs and filter those within the last 30 days
          RUNS=$(gh run list --workflow $WORKFLOW_ID --limit 100 --json databaseId,createdAt,status \
            --jq ".[] | select(.status==\"completed\" and .createdAt >= \"$CUTOFF_DATE\") | .databaseId" \
            --repo ${{ github.repository }})

          if [ -z "$RUNS" ]; then
            echo "No completed workflow runs in the last 30 days."
            exit 0
          fi

          # Loop through each run and download artifacts
          for RUN_ID in $RUNS; do
            echo "Downloading artifacts from workflow run: $RUN_ID"
            gh run download $RUN_ID --dir all-test-results || echo "No artifacts found for $RUN_ID"
          done
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}


      - name: Install junitparser for Merging JUnit XML
        run: pip install junitparser

      - name: Merge JUnit XML Reports
        run: |
          mkdir -p merged-results
          python - <<EOF
          from junitparser import JUnitXml, TestSuite, TestCase
      
          merged_xml = JUnitXml()
          merged_suites = TestSuite(name="Merged Test Suites")
      
          import glob
          test_files = glob.glob("all-test-results/**/*.xml", recursive=True)
      
          if not test_files:
              print("No JUnit XML files found to merge.")
              exit(1)
      
          total_tests = 0
          total_failures = 0
          total_errors = 0
          total_skipped = 0
          total_time = 0.0
      
          for file in test_files:
              print(f"Processing: {file}")
              xml = JUnitXml.fromfile(file)
      
              for suite in xml:
                  total_tests += suite.tests
                  total_failures += suite.failures
                  total_errors += suite.errors
                  total_skipped += suite.skipped
                  total_time += float(suite.time) if suite.time else 0.0
                  
                  # Append test cases individually
                  for case in suite:
                      merged_suites.add_testcase(case)
      
          # Set final counts in the merged test suite
          merged_suites.tests = total_tests
          merged_suites.failures = total_failures
          merged_suites.errors = total_errors
          merged_suites.skipped = total_skipped
          merged_suites.time = total_time
      
          merged_xml.add_testsuite(merged_suites)
      
          output_file = "merged-results/merged-test-results.xml"
          merged_xml.write(output_file)
          print(f"Merged JUnit XML report saved to: {output_file}")
          EOF
          
      - name: Upload Merged JUnit XML Report
        uses: actions/upload-artifact@v4
        with:
          name: merged-junit-test-report
          path: merged-results/merged-test-results.xml
          retention-days: 30

      - name: Install dependencies for analysis
        run: pip install junitparser matplotlib pandas
        
      - name: Parse JUnit XML and Generate Chart
        run: |
          python - <<EOF
          import matplotlib.pyplot as plt
          import pandas as pd
          from junitparser import JUnitXml

          # Load JUnit XML
          xml = JUnitXml.fromfile("merged-results/merged-test-results.xml")

          # Initialize counters
          passed, failed, errors, skipped = 0, 0, 0, 0
          unique_test_cases = set()

          # Process each test case
          for suite in xml:
              for case in suite:
                  unique_test_cases.add(case.name)  # Track unique test names
                  if case.result:
                      if any(isinstance(r, junitparser.Failure) for r in case.result):
                          failed += 1
                      elif any(isinstance(r, junitparser.Error) for r in case.result):
                          errors += 1
                      elif any(isinstance(r, junitparser.Skipped) for r in case.result):
                          skipped += 1
                  else:
                      passed += 1  # No result means it passed

          # Create DataFrame for visualization
          statuses = ["Passed", "Failed", "Errors", "Skipped"]
          counts = [passed, failed, errors, skipped]

          # Generate Pie Chart
          plt.figure(figsize=(8, 6))
          plt.pie(counts, labels=statuses, autopct='%1.1f%%', colors=['green', 'red', 'orange', 'gray'], startangle=140)
          plt.title("Test Status Breakdown")

          # List unique test cases below the chart
          plt.figtext(0.5, -0.1, "Unique Test Cases:\n" + "\n".join(unique_test_cases), wrap=True, horizontalalignment='center', fontsize=10)

          # Save chart
          plt.savefig("test-status-chart.png", bbox_inches='tight')

          print("Test Summary:")
          print(f"Passed: {passed}, Failed: {failed}, Errors: {errors}, Skipped: {skipped}")
          print("\nUnique Test Cases:")
          for test in unique_test_cases:
              print(test)
          EOF   
          
      - name: Upload Test Execution Chart
        uses: actions/upload-artifact@v4
        with:
          name: test-status-chart
          path: test-status-chart.png
